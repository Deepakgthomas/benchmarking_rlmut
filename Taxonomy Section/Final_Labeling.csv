ID,html_link,Rater_1_Label,Rater_1_Comments,Rater_2_Label,Rater_2_Comments,Rater_3_Label,Rater_3's Comments,Rater_4_Label,Rater_4_Comments,Rater_5_Label,Rater_5_Comments,List of Labels,Final Taxonomy Label
GH 2.1,https://github.com/ignc-research/arena-rosnav/commit/b8cd8d12674fa31c626e4d2a43d087bc272d2768,Suboptimal Reward Function,"flatland move model fix, more efficient reward calculation, episode r‚Ä¶
‚Ä¶eset fix",Suboptimal Reward Function,"To check: we need an adjective in this label, maybe ""suboptimal"" as it seems there is a reward function, it just gets changed. There are changes other than ""reward"", but I could not tie them to anything",Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal Reward Function,,Unrelated,Suboptimal Reward Function
GH 2.2,https://github.com/ignc-research/arena-rosnav/commit/b8cd8d12674fa31c626e4d2a43d087bc272d2768,Missing reset/close environment,,Missing reset/close environment,,Missing reset/close environment,,Missing reset/close environment,,Missing reset/close environment,,Missing stepping the environment,Missing reset/close environment
GH 4,https://github.com/Farama-Foundation/HighwayEnv/commit/008f01ea03a386494372f82d61e32df1ac8aeb91,Suboptimal Reward Function,"I think this is reward function not suboptimal reward shaping 
Fix episode termination reward in highway environment
",Suboptimal Reward Function,"I do not see how this is about shaping. Also, there is a change in is_terminal. Could that indicate smth about terminal state? ",Suboptimal Reward Function,,Suboptimal Reward Function,design?,Suboptimal Reward Function,,Suboptimal learning rate,Suboptimal Reward Function
GH 5,https://github.com/utiasDSL/safe-control-gym/commit/bd911747bdf0a87969333376cc9f732c43bbeeeb,Suboptimal Reward Function,"Adding obs_goal_horizon to cartpole and fixing reward
",Suboptimal Reward Function,"This seems to add smth about observation_goal_horizon,  but also changes the reward",Suboptimal Reward Function,,Suboptimal Reward Function,they now have 2 tasks and reeward was not adapted,Suboptimal Reward Function,,Missing terminal state,Suboptimal Reward Function
GH 7,https://github.com/ugr-sail/sinergym/issues/241,Suboptimal Reward Function,Reward Function,Suboptimal Reward Function,"I am not 100% sure about this, there is a relation to reward, but it is not very direct. The thing that convinces me is that they have assigned the tag ""reward"" to this bug. ",Suboptimal Reward Function,,Suboptimal Reward Function,I agree with Rater_3 it is similar to the issue where we had mapping to the speed,Suboptimal Reward Function,,Missing reset/close environment,Suboptimal Reward Function
GH 17,https://github.com/HumanCompatibleAI/imitation/commit/9526298f8d43c719bdb027f4e14691f860d46bc1,Unnecessary reward normalisation,"This is unclear to me
* Fix bugs
* fix bugs
* fix min_{timesteps=>episodes}",Unnecessary reward normalisation,"Huge commit, the commit title says smth about reward normalisation, but I could not locate the code that changes normalisation in an obvious way",Unnecessary reward normalisation,Normalization/Scale?,Unnecessary reward normalisation,,Unnecessary reward normalisation,,Missing exploration,Unnecessary reward normalisation
GH 18,https://github.com/HumanCompatibleAI/imitation/commit/04e1ed61c6332768a549f4e0c0b3d996428791c1,Wrapping reward before normalization,"* Bugfix due to steps => I see code for this but no idea what's going on
* Bugfix: apply reward wrapping before normalizing observations
",Wrapping reward before normalization,"I can see the code for ""apply reward wrapping before normalizing observations"". Does this lead to changes in reward scale? We need an adjective for this label too.",Wrapping reward before normalization,,Wrapping reward before normalization,,Wrapping reward before normalization,,Suboptimal exploration rate,Wrapping reward after observation normalisation
GH 19,https://github.com/DLR-RM/stable-baselines3/commit/dd6e361204e24932184a4a6b05ce3b91d29671c7,Selecting a non-deterministic trained policy,"      action, _states = model.predict(obs, deterministic=True)",Selecting a non-deterministic trained policy,,Selecting a non-deterministic trained policy,,Selecting a non-deterministic trained policy,,Selecting a non-deterministic trained policy,,Wrong update rule,Misconfiguration of the agent to a stochastic inference
GH 22,https://github.com/HumanCompatibleAI/imitation/commit/005c15fe737ac558d13aab15037cf9615ca8a83b,Missing check for dones,"* Fixed bug in buffer.py
* Fix not checking for dones
* Fixed n_steps (OnPolicyAlgorithm)
* Fix density.py::DensityAlgorithm._set_demo_from_batch
* Fix issue with next(iter(iterable))
* Fix_get_first_iter_element and add tests
* Bugfix in BC and tests -- masked as previously iterator ran out too early!
",Missing check for dones,,Missing check for dones,,Missing check for dones,,Missing check for dones,,Suboptimal network update frequency,Missing check for dones
GH 24,https://github.com/DLR-RM/rl-baselines3-zoo/commit/312724f7e7efad18c1393a22cf1ca4ef08573448,Missing reward normalisation,"* Fix reward normalization when doing hyperparameter optimization
* Fix reward norm
",Missing reward normalisation,,Missing reward normalisation,Normalization/Scale?,Missing reward normalisation,Normalisation of reward?,Missing reward normalisation,,Wrong network update,Missing reward normalisation/scaling
GH 28,https://github.com/HumanCompatibleAI/imitation/pull/184,Pontential of terminal step not fixed to zero,,Pontential of terminal step not fixed to zero,,Pontential of terminal step not fixed to zero,,Pontential of terminal step not fixed to zero,,Pontential of terminal step not fixed to zero,,Wrong calculation of gradients,Potential of terminal step not fixed to zero
GH 32,https://github.com/DLR-RM/stable-baselines3/issues/597,Suboptimal network update frequency,,Suboptimal network update frequency,Can't find things related to the other 3 labels,Suboptimal network update frequency,,Suboptimal network update frequency,,Suboptimal network update frequency,,Wrong output,Suboptimal network update frequency
GH 34,https://github.com/DLR-RM/stable-baselines3/issues/597,Incorrect learning_start,,Incorrect learning_start,,Incorrect learning_start,,Incorrect learning_start,,Incorrect learning_start,"""learning_starts and target_update_interval are above each budget total_timesteps for individual call for .learn()""",Wrong activation for output,Suboptimal learning start 
GH 36,https://github.com/DLR-RM/stable-baselines3/pull/198,Biased target network,"Framework issue, but is a good issue",Biased target network,"To check: should we make it more precise to underline that it is the features that are shared. ""Fixes the issue that the feature extractor was shared between network and target.""",Biased target network,,Biased target network,not sure what is feature extractor,Biased target network,* Fix feature extractor bug,Unclear,Biased target network
GH 38,https://github.com/DLR-RM/stable-baselines3/issues/627,Incorrect done flag,,Incorrect done flag,To check: should we come up with a better label?,Incorrect done flag,,Incorrect done flag,,Incorrect done flag,,Reward Function,"Flags indicating successful termination or
timeout not set properly "
GH 42,https://github.com/HumanCompatibleAI/imitation/pull/335,Incorrect done flag,,Incorrect done flag,,Incorrect done flag,RL with infinite horizon,Incorrect done flag,,Incorrect done flag,,Replay Buffer Design [Ignore this],"Flags indicating successful termination or
timeout not set properly "
GH 43,https://github.com/HumanCompatibleAI/imitation/pull/323,Pontential of terminal step not fixed to zero,What's the issue over here,Pontential of terminal step not fixed to zero,It is not clear to me,Pontential of terminal step not fixed to zero,RL with infinite horizon: https://github.com/HumanCompatibleAI/imitation/pull/323/commits/b06d354d507022491d332999a128b295891a1459,Pontential of terminal step not fixed to zero,not sure,Pontential of terminal step not fixed to zero,re-write the label,Non MDP Problem Formulation,Potential of terminal step not fixed to zero
GH 45,https://github.com/vwxyzjn/cleanrl/pull/209,Discount Factor,This is discount factor,Discount Factor,"There is no description for reward shaping label. The suboptimal reward shaping's description does not seem to match this situation. Also, is not gamma the discount factor. We need an adjective for this label.",Discount Factor,,Discount Factor,,Discount Factor,,Incomplete State Description,Suboptimal discount factor 
GH 49,https://github.com/HumanCompatibleAI/imitation/pull/43,Not enough episodes/iterations (training),"This should be timesteps, I guess",Not enough episodes/iterations (training),,Not enough episodes/iterations (training),,Not enough episodes/iterations (training),,Not enough episodes/iterations (training),,Suboptimal Discount Factor,Not enough episodes/iterations (training)
GH 50,https://github.com/DLR-RM/stable-baselines3/commit/0c50d75ecb6287132c9de4d7070e50905c5f632d,Missing normalisaton of observations,*,Missing normalisaton of observations,"Big commit, the textual description does not say anything about reward. The changelog says ""this allows to return the unnormalized reward in the case of Atari games for instance"". Not sure I can identify the reward scale issue.",Missing normalisaton of observations,(big commit) https://github.com/DLR-RM/stable-baselines3/commit/0c50d75ecb6287132c9de4d7070e50905c5f632d,Missing normalisaton of observations,,Missing normalisaton of observations,"Many commits, I dont see anything related to reward but it is clear * Fix terminal observation issues
",Training Diversity,Suboptimal / Missing normalisaton of observations
GH 53,https://github.com/HumanCompatibleAI/imitation/pull/25,Missing replay buffer,,Missing replay buffer,,Missing replay buffer,Feature?,Missing replay buffer,,Missing replay buffer,,Large action space,Dropped
GH 55,https://github.com/eugenevinitsky/sequential_social_dilemma_games/commit/227e3be69c40f6df2e874cb52e36d6f29b9730a6,Suboptimal Reward Function,,Suboptimal Reward Function,need an adjective,Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal Reward Function,,Small number of MCTS simulations,Suboptimal Reward Function
GH 57,https://github.com/freqtrade/freqtrade/pull/7809,Non random start point,Great motivation for this label,Non random start point,need to rename the label?,Non random start point,,Non random start point,,Non random start point,very sprcific label :selecting random start point,Missing normalisaton of observations,Non-random starting state 
GH 58,https://github.com/Farama-Foundation/HighwayEnv/issues/75,Suboptimal Environment Dynamics,Environment dynamics may be better,Suboptimal Environment Dynamics,need an adjective? ,Suboptimal Environment Dynamics,,Suboptimal Environment Dynamics,,Suboptimal Environment Dynamics,,Suboptimal Environment Dynamics,Drop
GH 60,https://github.com/HumanCompatibleAI/imitation/pull/425,Missing reward normalisation,,Missing reward normalisation,,Missing reward normalisation,,Missing reward normalisation,,Missing reward normalisation,,Not enough episodes/iterations (training),Missing reward normalisation/scaling 
GH 61,https://github.com/lebrice/Sequoia/pull/88,Replay Buffer Design,,Replay Buffer Design,"I can see that they fix things related to batching, but I am not completely sure this is related to buffer design",Replay Buffer Design,,Replay Buffer Design,,Replay Buffer Design,"one fix: Change the return of Batch.slice(indices) to always have a batch dimension,",Missing target network,Suboptimal replay buffer design 
GH 62,https://github.com/DLR-RM/stable-baselines3/issues/456,Missing reset/close environment,,Missing reset/close environment,,Missing reset/close environment,,Missing reset/close environment,,Missing reset/close environment,,Incorrect State Description,Missing reset/close environment 
GH 63,https://github.com/DLR-RM/stable-baselines3/issues/76,Wrong network update,Framework bug,Wrong network update,,Wrong network update,Difference with wrong network update?,Wrong network update,,Wrong network update,"[Bug]: Issue with SubprocVecEnv when observation space is a Dict #1428
https://github.com/DLR-RM/stable-baselines3/issues/1428",Suboptimal exploration rate - OU Coefficient,Wrong network update / Wrong update rule
GH 65,https://github.com/ugr-sail/sinergym/issues/208,Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal exploration rate - Epsilon,Suboptimal reward function
GH 67,https://github.com/HumanCompatibleAI/imitation/pull/342,Non stochastic policy for AIRL,"""More fundamentally, AIRL needs a stochastic policy."" => I think this is relevant. But I feel a different labels needs to be used",Non stochastic policy for AIRL,Seems like a new feature being added,Non stochastic policy for AIRL,Feature implementation,Non stochastic policy for AIRL,,Non stochastic policy for AIRL,,Suboptimal exploration rate - Epsilon,Non-stochastic policy
GH 68,https://github.com/HumanCompatibleAI/imitation/pull/333,Training on variable horizon tasks,,Training on variable horizon tasks,To check: we probably need to rename this label into smth better,Training on variable horizon tasks,,Training on variable horizon tasks,,Training on variable horizon tasks,Variable horizon tasks cause all sorts of problems as discussed in #324,Suboptimal initialization state,"Use of variable horizon in environment where fixed
horizon is more appropriate "
GH 69,https://github.com/HumanCompatibleAI/imitation/pull/256,Missing normalisaton of observations,"""I introduce another VecNormalize to normalize (and clip) rewards but not observations after the wrapped reward."" => Maybe normalize and clip rewards should be also included


2. Also this is important => ""I added keyword arguments that let you control whether to normalize observations or rewards. There are some environments in particular where normalizing observations is unnecessary or undesirable (e.g. Atari).""",Missing normalisaton of observations,,Missing normalisaton of observations,,Missing normalisaton of observations,,Missing normalisaton of observations,,Deep Learning Fault,Suboptimal / Missing normalisaton of observations
GH 70,https://github.com/HumanCompatibleAI/imitation/pull/130,Reward Scale,,Reward Scale,,Reward Scale,VecNormalize,Reward Scale,,Reward Scale,training with an unnormalized reward,Reward Scale,Missing reward normalisation/scaling 
GH 71,https://github.com/HumanCompatibleAI/imitation/pull/130,Missing normalisaton of observations,,Missing normalisaton of observations,,Missing normalisaton of observations,(duplicate) VecNormalize,Missing normalisaton of observations,,Missing normalisaton of observations,,Wrong Generalization Approach,Suboptimal / Missing normalisaton of observations
GH 72,https://github.com/Farama-Foundation/HighwayEnv/pull/110,Suboptimal reward mapping,,Suboptimal reward mapping,"This is about reward, but not sure about ""mapping"". The description of the label is a link atm.",Suboptimal reward mapping,Feature implementation,Suboptimal reward mapping,,Suboptimal reward mapping,"I dont see bug, the developer asked question: Reward in highway-env with continuous control (https://github.com/Farama-Foundation/HighwayEnv/issues/109)",Suboptimal Reward Shaping,"Reward function not defined for entire range
of behaviour "
GH 73,https://github.com/glmcdona/LuxPythonEnvGym/pull/86,Suboptimal Reward Function,"1. ""Set default example agent to inference on non-deterministic mode. Agents often get stuck when set to deterministic in inference.""
",Suboptimal Reward Function,need an adjective,Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal Reward Function,,Incorrect done flag,Suboptimal reward function
GH 73,https://github.com/glmcdona/LuxPythonEnvGym/pull/86,Misconfiguration of the agent to a deterministic inference,,Misconfiguration of the agent to a deterministic inference,,Misconfiguration of the agent to a deterministic inference,,Misconfiguration of the agent to a deterministic inference,,Misconfiguration of the agent to a deterministic inference,,Large State Description,Misconfiguration of the agent to a deterministic inference
GH 75,https://github.com/deepmind/open_spiel/pull/43,Missing gradient clipping,,Missing gradient clipping,,Missing gradient clipping,,Missing gradient clipping,,Missing gradient clipping,Without gradient clipping the PolicyGradient agent was unstable,Sparse Reward,Missing policy-gradient clipping 
GH 77,https://github.com/HumanCompatibleAI/imitation/pull/110,Suboptimal observation design,"""This makes the environment highly non-Markovian, and leads to even expert policies looking rather jagged: they keep accelerating towards the particle, overshooting, and correcting winding up in SHM-like motion."" => Non Markovian Environment",Suboptimal observation design,,Suboptimal observation design,,Suboptimal observation design,,Suboptimal observation design,I am not sure if it is bug or not,Incorrect use of seed,Drop
GH 81,https://github.com/DLR-RM/stable-baselines3/issues/440,Normalizing with a single sample,,Normalizing with a single sample,To check: is this RL-specific?,Normalizing with a single sample,(duplicate of GH 12) Advantage estimation calculation in PPO should not depend on the batch size,Normalizing with a single sample,not sure about this one,Normalizing with a single sample,"""One way would be to throw an error when batch_size=1 or allow to deactivate advantage normalization""",Wrong input to compute returns,Wrong normalisation of advantage 
GH 88,https://github.com/HumanCompatibleAI/imitation/pull/167,Missing normalisaton of observations,,Missing normalisaton of observations,,Missing normalisaton of observations,,Missing normalisaton of observations,,Missing normalisaton of observations,,Incorrect learning_start,Suboptimal / Missing normalisaton of observations
GH 89,https://github.com/HumanCompatibleAI/imitation/pull/54,Suboptimal number of n_steps,,Suboptimal number of n_steps,"""Increases the default n_steps collected by PPO to be consistent with.."". From the documentation ""(i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel)""",Suboptimal number of n_steps,,Suboptimal number of n_steps,,Suboptimal number of n_steps,"""Fixes Wrapping VecEnv -- Final (old_obs, act, obs) triple is incorrect. #1 by recording terminal transition during rollouts.""",Biased target network,Suboptimal number of rollout steps 
GH 93,https://github.com/HumanCompatibleAI/imitation/pull/359,Missing reset/close environment,,Missing reset/close environment,"""So it could happen that we had partial trajectories, which would be collected by pop_trajectories, but when training later resumed, it resumed from the middle of an episode. """,Missing reset/close environment,,Missing reset/close environment,,Missing reset/close environment,"""The issue was that we use reset_num_timesteps=False when calling SB3's learn(), which has the side effect of not resetting the environment""",Missing check for dones,Missing reset/close environment
GH 95,https://github.com/utiasDSL/gym-pybullet-drones/pull/32,Suboptimal action space design,"Maybe even reward and dones - 

fix _computeDone function to reduce unnecessary interaction with environment.
fix _computeReward function to enhance agent to go higher than before.",Suboptimal action_space design,To check: should we rename this?,Suboptimal action_space design,Action space design? There is no mention in the post that the action space is large. There is also the fix of the reward function that could be considered as post to label.,Suboptimal action_space design,,Suboptimal action space design,"there are 3 fixes: fix the range of action space from -1 ~ 1 to 0 ~ 1 for the orthogonal manipulation of the agent.
fix _computeDone function to reduce unnecessary interaction with environment.
fix _computeReward function to enhance agent to go higher than before.",Selecting a non-deterministic trained policy,Suboptimal action space
GH 95,https://github.com/utiasDSL/gym-pybullet-drones/pull/32,Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal Reward Function,,Incorrect episode length during inference,Suboptimal Reward Function
GH 95,https://github.com/utiasDSL/gym-pybullet-drones/pull/32,Incorrect done flag,,Incorrect done flag,,Incorrect done flag,,Incorrect done flag,,Incorrect done flag,,Missing online_sampling in replay buffer,"Flags indicating successful termination or
timeout not set properly"
GH 99,https://github.com/HumanCompatibleAI/imitation/pull/393,suboptimal observation normalisation,,suboptimal observation normalisation,,suboptimal observation normalisation,,suboptimal observation normalisation,,suboptimal observation normalisation,,Missing potential shaping*,Suboptimal / Missing normalisaton of observations
GH 100,https://github.com/HumanCompatibleAI/imitation/pull/393,missing action normalisation,"""When storing buffers of observations, it's important to store unnormalized observations and normalize just before feeding into the policy/reward network to avoid distribution shift. "" => What does this mean?",missing action normalisation,,missing action normalisation,Not sure what they mean with action normalization,missing action normalisation,,missing action normalisation,"Is this cause bug? ""When storing buffers of observations, it's important to store unnormalized observations and normalize just before feeding into the policy/reward network to avoid distribution shift. This and related problems were a common source of bugs.""",Missing replay buffer,Missing action normalisation 
GH 101,https://github.com/contactrika/dedo/commit/d12bfb50bcf4718e2b5907a29f1a97e52906634c,Missing normalisaton of observations,"Observation normalization is important here too -

Fixed reward issues (restored penalty for eary termination); fixed im‚Ä¶
‚Ä¶age obs not being in [0,1];",Missing normalisaton of observations,,Missing normalisaton of observations,,Missing normalisaton of observations,could you find it in the code,Missing normalisaton of observations,Your reward function makes it painful to be alive,Pontential of terminal step not fixed to zero,Suboptimal / Missing normalisaton of observations
SE 1,https://ai.stackexchange.com/questions/7940,Suboptimal Reward Function,,Suboptimal Reward Function,"At the very end of the discussion (in the comments) the person says ""the scikit-learn fit method was the problem I had to use partial_fit instead thanks a lot now I seem to get some  results, the snake finally learns to go after the apple, I still need to modify the reward function for better performance according to your suggestions""",Suboptimal Reward Function,It could also be a DL problem,Suboptimal Reward Function,,Suboptimal Reward Function,,Training on variable horizon tasks,Suboptimal Reward Function
SE 5,https://ai.stackexchange.com/questions/10830,Suboptimal sampling from replay buffer,,Suboptimal sampling from replay buffer,"""redoing my experience selection method"" - is this about replay buffer design?",Suboptimal sampling from replay buffer,,Suboptimal sampling from replay buffer,,Suboptimal sampling from replay buffer,,Unnecessary reward normalisation,Suboptimal sampling from replay buffer
SE 6,https://ai.stackexchange.com/questions/11433,Wrong update rule,Should be wrong update rule,Wrong update rule,To check. Is there a clear difference between the two labels?,Wrong update rule,,Wrong update rule,,Wrong update rule,,Missing reward normalisation,Wrong network update / Wrong update rule
SE 7,https://ai.stackexchange.com/questions/11987,Small number of MCTS simulations,"*Action space will be more generalizable

Maybe large branching factor",Small number of MCTS simulations,To check. Is this too precise?,Small number of MCTS simulations,,Small number of MCTS simulations,,Small number of MCTS simulations,,Selecting a non-stochastic trained policy,Small number of mcts simulations 
SE 9,https://ai.stackexchange.com/questions/15910,Wrong update rule,Should be wrong update rule,Wrong update rule,The solution seems to be the change of learning rate,Wrong update rule,,Wrong update rule,"Not sure about the label (missing LR decay), but this lr is not for DL network, but for the Q table update.",Wrong update rule,I was not decaying the learning rate appropriately,Suboptimal termination condition,Wrong network update / Wrong update rule
SE 10,https://ai.stackexchange.com/questions/17538,Suboptimal Reward Function,,Suboptimal Reward Function,"Note: there is no confident answer here. We can keep the label because the accepted answer says ""instead of just penalizing the model for failing, you should reward if for winning"". The comment to the answer by the person who asked the question ""The problem was in the state and the reward"". ",Suboptimal Reward Function,,Suboptimal Reward Function,The problem was in the state and the reward,Suboptimal Reward Function,"you should reward if for winning, so pausing is no longer the best option.",,Suboptimal reward function
SE 12,https://ai.stackexchange.com/questions/20858,Suboptimal Discount Factor,*,Suboptimal Discount Factor,To check: it seems discount factor is only part of the problem. There is quite some discussion on is_slippery and environmental traces.,Suboptimal Discount Factor,,Suboptimal Discount Factor,,Suboptimal Discount Factor,discount rate (set for 0.99) was causing these errors.,Reward clip ,Suboptimal discount factor
SE 14,https://ai.stackexchange.com/questions/21872,Suboptimal observation design,Generalization approach is missing here,Suboptimal observation design,"It seems to me like a rounding error, which leads to an incorrect values for observation space. Not sure what is the best label for this. ",Suboptimal observation design,,Suboptimal observation design,,Suboptimal observation design,"More than one problem. The problem here is likely related to the state approximations you are using,  The approximation approach of discrete grid is also very crude",Incorrect batch size,Drop
SE 15,https://ai.stackexchange.com/questions/22201,Suboptimal Discount Factor,"1. I think this should have something about problem formulation
2. Also non random start point is applicable here (In your evaluation code (after training), I believe that you always make the Random opponent play first, and the trained agent play second? If you don't cover this scenario in your training episodes, your agent may not learn how to properly handle it.)",Suboptimal Discount Factor,"Note: the part about discount factor is a ""small note"". ",Suboptimal Discount Factor,Seems to be an issue with the way the agents are updated in multi-agent setting.,Suboptimal Discount Factor,,Suboptimal Discount Factor,"Your discount factor ùõæ0.1 has an extremely small value, always make the Random opponent play first, and the trained agent play second? If you don't cover this scenario in your training episodes, your agent may not learn how to properly handle it.",Missing gradient clipping,Suboptimal discount factor 
SE 15 [Duplicate],https://ai.stackexchange.com/questions/22201,Non random start point,,Non random start point,,Non random start point,,Non random start point,,Non random start point,,suboptimal number of rollout buffer steps,Non-random starting state
SE 16,https://ai.stackexchange.com/questions/22945,Suboptimal Replay buffer size,"There are many changes here -
1. Sampling the noise 
2. Changing polyak 
3. Changed hidden layer sizes
4. Replay buffer design -> Buffer size (to be specific)
5. Batch size",Suboptimal Replay buffer size,"There are many changes, but this one seems to be the only one that is definitely RL related",Suboptimal Replay buffer size,Changed the polyak constant (tau) from 0.99 to 0.001; other changes as well,Suboptimal Replay buffer size,alsso the constant?,Suboptimal Replay buffer size,I see many changes (change the actions selection function for this and tune some hyper-parameters.),,Suboptimal Replay buffer size
SE 16,https://ai.stackexchange.com/questions/22945,Suboptimal Polyak constant value,,Suboptimal Polyak constant value,,Suboptimal Polyak constant value,,Suboptimal Polyak constant value,,Suboptimal Polyak constant value,,,Suboptimal Polyak constant value
SE 16,https://ai.stackexchange.com/questions/22945,Suboptimal noise sampling,,Suboptimal noise sampling,,Suboptimal noise sampling,,Suboptimal noise sampling,,Suboptimal noise sampling,,,Suboptimal noise sampling
SE 18,https://ai.stackexchange.com/questions/23299,Suboptimal minimum exploration rate,"1. Set minimum exploration rate
2. Generalization approach (More convolutional layers)
3. State representation
4. Increase number of iterations

I have the same observation as Reviewer_5 that exploration and state representation. In addition, I have 2 more.",Suboptimal minimum exploration rate,"To check. There are two more changes: remove dropout and more conv layers, but this is DL related. Also, smth about state representation and normalisation. ",Suboptimal minimum exploration rate,,Suboptimal minimum exploration rate,,Suboptimal minimum exploration rate,"three, maybe four, things in your implementation that could be contributing to incomplete learning that you are observing.
Suboptimal exploration rate
state representation
dropout in RL neural networks
More convolutional layers
",,Suboptimal minimum exploration rate
SE 18,https://ai.stackexchange.com/questions/23299,Not enough episodes/iterations (training),,Not enough episodes/iterations (training),,Not enough episodes/iterations (training),,Not enough episodes/iterations (training),,Not enough episodes/iterations (training),,,Not enough episodes/iterations (training) 
SE 18,https://ai.stackexchange.com/questions/23299,Incorrect State Description,,Incorrect State Description,,Incorrect State Description,,Incorrect State Description,,Incorrect State Description,,,Suboptimal State Space
SE 20,https://ai.stackexchange.com/questions/23518,Deep Learning Fault,,Deep Learning Fault,To check. Is this not DL related?,Deep Learning Fault,,Deep Learning Fault,MENTION IN THE PAPER WHY WE DROPPED THIS LABEL,Deep Learning Fault,,evaluation of stochastic trained agents,Drop
SE 21,https://ai.stackexchange.com/questions/23810,Suboptimal Replay Buffer Design,,Suboptimal Replay Buffer Design,"""Keep aside some percentage of replay memory stocked with the initial 
poor performing random exploration. Reserving say 10% to this long term 
memory is enough to make learning in Cartpole rock solid,",Suboptimal Replay Buffer Design,Catastrophic forgetting is also common in DL and NN in general. Could also be exploration rate.,Suboptimal Replay Buffer Design,the hack with replay buffer is just workaround as I understand,Suboptimal Replay Buffer Design,I found a very simple hack made the learning very stable. Keep aside some percentage of replay memory stocked with the initial poor performing random exploration.,,Suboptimal replay buffer design 
SE 22,https://ai.stackexchange.com/questions/24194,Incorrect State Description,*,Incorrect State Description,All the discussion revolves around the structure of state matrix and the answer is accepted,Incorrect State Description,"It seems that the qtable is wrongly implemented, i.e., it does not take into account the state space",Incorrect State Description,,Incorrect State Description,,Suboptimal observation design,Suboptimal State Space
SE 23,https://ai.stackexchange.com/questions/25939,Suboptimal application of Dirichilet noise,*,Suboptimal application of Dirichilet noise,Unclear to me),Suboptimal application of Dirichilet noise,,Suboptimal application of Dirichilet noise,,Suboptimal application of Dirichilet noise,,Non random start point,Suboptimal application of Dirichilet noise
SE 24,https://ai.stackexchange.com/questions/27593,Missing normalisaton of observations,,Missing normalisaton of observations,"The comment says ""I tried normalizing my observations between -1 and 1 and this caused a major improvement.""",Missing normalisaton of observations,,Missing normalisaton of observations,,Missing normalisaton of observations,I tried normalizing my observations between -1 and 1 and this caused a major improvement. I might also try this for the rewards and actions next,Suboptimal reward mapping,Suboptimal / Missing normalisaton of observations
SE 25,https://ai.stackexchange.com/questions/35537,Not enough episodes/iterations (training),Reward function,Not enough episodes/iterations (training),"The answer to the answer in the comment says ""I run the 50 million episodes (yes, I'm working with episodic tasks) and it worked"". The answer is mostly suggestions that I can not tie to clear labels",Not enough episodes/iterations (training),,Not enough episodes/iterations (training),,Not enough episodes/iterations (training),"There are lots of changes you might consider to improve your agent to deal with harder-to-solve environments in reasonale time. I am not sure what you are able to consider, so here are a few open-ended ideas:",suboptimal observation normalisation,Not enough episodes/iterations (training) 
SE 29,https://ai.stackexchange.com/questions/37085,Missing target network,,Missing target network,"""de-correlate target and action value using two model""",Missing target network,,Missing target network,,Missing target network,,missing action normalisation,Drop
SE 30,https://datascience.stackexchange.com/questions/24598,Missing replay buffer,,Missing replay buffer,"I am assuming here that the point about ""experience replay"" affects replay buffer design. That being said, there is no clear confirmation that applying this suggestion solved the problem",Missing replay buffer,,Missing replay buffer,,Missing replay buffer,,Missing evaluation environment,Drop
SE 30,https://datascience.stackexchange.com/questions/24598,Missing target network,,Missing target network,"I am assuming here that the point about ""Frozen bootstrap target network"" affects network updates. That being said, there is no clear confirmation that applying this suggestion solved the problem",Missing target network,,Missing target network,,Missing target network,,Suboptimal Reward Function,Drop
SE 31 [duplicate],https://datascience.stackexchange.com/questions/37081,Incorrect State Description,,Incorrect State Description,"""You should do that - without it the replay table will be filled with incorrect values"". Would replay buffer design be a better label here?",Incorrect State Description,,Incorrect State Description,,Incorrect State Description,,,Suboptimal State Space
SE 31 [duplicate],https://datascience.stackexchange.com/questions/37081,Missing reset/close environment,,Missing reset/close environment,"""You are running the system as an episodic problem, but do not re-set the environment for the start of a new episode.""",Missing reset/close environment,,Missing reset/close environment,,Missing reset/close environment,,,Missing reset/close environment 
SE 34,https://datascience.stackexchange.com/questions/80417,Incomplete State Description,,Incomplete State Description,To check. There seems to be a clear problem with the set of features that is being used. Then there is a suggestion to normalise them. ,Incomplete State Description,,Incomplete State Description,,Incomplete State Description,,Suboptimal Replay Buffer Design,Suboptimal State Space
SE 36,https://stackoverflow.com/questions/30841373,Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal Reward Function,,Not wrapping reward before obervation normalization,Suboptimal Reward Function
SE 36 [Duplicate],https://stackoverflow.com/questions/30841373,Discount Factor,,Suboptimal Discount Factor,,Suboptimal Discount Factor,,Suboptimal Discount Factor,,Suboptimal Discount Factor,,Non stochastic policy for AIRL,Suboptimal discount factor
SE 36 [Duplicate],https://stackoverflow.com/questions/30841373,Suboptimal exploration rate - Epsilon,,Suboptimal exploration rate - Epsilon,,Suboptimal exploration rate - Epsilon,,Suboptimal exploration rate - Epsilon,,Suboptimal exploration rate - Epsilon,,Misconfiguration of the agent to a deterministic inference,Suboptimal exploration rate - Epsilon 
SE 37,https://stackoverflow.com/questions/48476039,Suboptimally balancing value and policy networks,,Suboptimally balancing value and policy networks,"Don't see anything about exploration rate here. There is a suggestion to change the value of lambda, but it is not clear if that fixes anything",Suboptimally balancing value and policy networks,,Suboptimally balancing value and policy networks,,Suboptimally balancing value and policy networks,,Normalizing with a single sample,"Suboptimally balancing value and policy
networks"
SE 39,https://stackoverflow.com/questions/64778628,Suboptimal Reward Function,,Suboptimal Reward Function,"To check. The accepted answer is about changing reward, but in the comment the person who asked the question suggest that they will switch to curriculum learning instead, which might mean switching the algorithm",Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal number of n_steps,Suboptimal Reward Function
SE 40,https://stackoverflow.com/questions/67789148,Discount Factor,,Discount Factor,We need to rename this label,Discount Factor,,Discount Factor,,Discount Factor,,Suboptimal action space design,Suboptimal discount factor 
SE 41,https://stackoverflow.com/questions/15693545,Missing reward normalisation,1. Reward scale would be more specific,Missing reward normalisation,"In the comment it is said ""Using the log method did not help, but your idea lead to another idea: I found a way to normalize the reward to [-1,1] which made things much easier'",Missing reward normalisation,,Missing reward normalisation,,Missing reward normalisation,I suggest that use log of rewards. This is a standard trick in math to control huge values.,Suboptimal sampling from replay buffer,Missing reward normalisation/scaling 
SE 43,https://stackoverflow.com/questions/53429724,Non random start point,"After I read this post again, this is not a case of suboptimal initialization. Rather it is - 

1. Non random start point
2. Suboptimal exploration",Non random start point,This is more of a suggestion without a proper evidence that this indeed fixes the problem,Non random start point,May also be exploration,Non random start point,,Non random start point,Suboptimal exploration more appropriate here. A basic case would be if the agent is not exploring enough and it is not that easy to get to the final state from the state you've chosen for initialization. ,Suboptimal Replay buffer size,Non-random starting state
SE 44,https://stackoverflow.com/questions/56816743,Missing replay buffer,"1. Batch size and no replay buffer
2. Increase iterations",Missing replay buffer,,Missing replay buffer,,Missing replay buffer,,Missing replay buffer,batch size is 1 which makes it even noisier. Batch sampling really did make a difference,Suboptimal Polyak constant value,Drop
SE 44 [Duplicate],https://stackoverflow.com/questions/56816743,Not enough episodes/iterations (training),,Not enough episodes/iterations (training),,Not enough episodes/iterations (training),,Not enough episodes/iterations (training),,Not enough episodes/iterations (training),,Suboptimal noise sampling,Not enough episodes/iterations (training) 
SE 45,https://stackoverflow.com/questions/57106676,Suboptimal network update frequency,Suboptimal target frequency to be more specific,Suboptimal network update frequency,,Suboptimal network update frequency,,Suboptimal network update frequency,,Suboptimal network update frequency,,Suboptimal minimum exploration rate,"Suboptimal network update
frequency "
SE 46,https://stackoverflow.com/questions/64441708,Missing reset/close environment,,Missing reset/close environment,"""After checking your provided code, the problem doesn't seem to come from what agent starts the game but from not restarting the environment after a game is done""",Missing reset/close environment,,Missing reset/close environment,,Missing reset/close environment,from not restarting the environment after a game is done,Suboptimal application of Dirichilet noise,Missing reset/close environment
SE 47 [Duplicate],https://stackoverflow.com/questions/71897010,Suboptimal network update frequency,"Maybe be more speciifc - 

1. Batch size
2. 10 batches in replay buffer => Maybe relevant",Suboptimal network update frequency,,Suboptimal network update frequency,,Suboptimal network update frequency,,Suboptimal network update frequency,there are many changes one of them change batch size,Suboptimally balancing value and policy networks,"Suboptimal network update
frequency "
SE 47 [Duplicate],https://stackoverflow.com/questions/71897010,Not enough episodes/iterations (training),I think this should be timesteps and not training iterations,Not enough episodes/iterations (training),,Not enough episodes/iterations (training),,Not enough episodes/iterations (training),,Not enough episodes/iterations (training),,Suboptimal decay of exploration,Not enough episodes/iterations (training) 
SE 48 [Duplicate],https://stackoverflow.com/questions/27340967,Missing replay buffer,,Missing replay buffer,,Missing replay buffer,,Missing replay buffer,,Missing replay buffer,,,Drop
SE 48 [Duplicate],https://stackoverflow.com/questions/27340967,Missing target network,,Missing target network,,Missing target network,,Missing target network,,Missing target network,,,Drop
SE 48 [Duplicate],https://stackoverflow.com/questions/27340967,Deep Learning Fault,,Deep Learning Fault,,Deep Learning Fault,,Deep Learning Fault,,Deep Learning Fault,,,Drop
SE 50,https://stackoverflow.com/questions/37409528,Incorrect State Description,Also maybe add generalization approach,Incorrect State Description,"Very generic explanation, but the answer says ""Your agent is doing the best it can given the state representation""",Incorrect State Description,,Incorrect State Description,,Incorrect State Description,,Subtoptimal frame skip parameter,Suboptimal State Space
SE 50 [Duplicate],https://stackoverflow.com/questions/37409528,Wrong Generalization Approach,,Wrong Generalization Approach,,Wrong Generalization Approach,,Wrong Generalization Approach,,Wrong Generalization Approach,,Suboptimal Entropy Coefficient,"Use of inappropriate function approximator for the
given environment "
SE 52,https://stackoverflow.com/questions/51425688,Suboptimal decay of exploration,Exploration decay may be more specific,Suboptimal decay of exploration,"""You might want to change your get_random_action function to decay epsilon with each episode.""",Suboptimal decay of exploration,,Suboptimal decay of exploration,,Suboptimal decay of exploration,Exploration decay more specific but we cannot create label for each cases ,,Suboptimal exploration decay
SE 55,https://stackoverflow.com/questions/69593105,Suboptimal Reward Function,,Suboptimal Reward Function,"""I solved this by giving a -0.1 reward on every step""",Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal Reward Function,,,Suboptimal reward function
SE 55 [Duplicate],https://stackoverflow.com/questions/69593105,Suboptimal exploration rate,,Suboptimal exploration rate,"""I also sped up the learning process by increasing the epsilon_greedy 
parameter of the DqnAgent's constructor to 0.5 (from it's default of 
0.1) to allow it to more quickly explore the entire environment.""",Suboptimal exploration rate,,Suboptimal exploration rate,,Suboptimal exploration rate,,Suboptimal features for state representation,Suboptimal exploration rate 
SE 58,https://stackoverflow.com/questions/45886398,Suboptimal exploration rate - Epsilon,,Suboptimal exploration rate - Epsilon,"I guess this relies on ""If you are never getting to the top you should probably increase your time doing exploration"", but I am not sure if this is enough",Suboptimal exploration rate - Epsilon,,Suboptimal exploration rate - Epsilon,,Suboptimal exploration rate - Epsilon,,Suboptimal scaling of features,Suboptimal exploration rate - Epsilon
SE 60,https://stackoverflow.com/questions/54385568,Suboptimal decay of exploration,,Suboptimal decay of exploration,smth more specific about decaying? ,Suboptimal decay of exploration,,Suboptimal decay of exploration,,Suboptimal decay of exploration,,,Suboptimal exploration decay
SE 61,https://stackoverflow.com/questions/56964657,Suboptimal network update frequency,,Suboptimal network update frequency,,Suboptimal network update frequency,,Suboptimal network update frequency,,Suboptimal network update frequency,,,"Suboptimal network update
frequency "
SE 62,https://stackoverflow.com/questions/65692797,Reward Scale,,Reward Scale,,Reward Scale,What is the difference with reward normalization?,Reward Scale,Suboptimal? :D,Reward Scale,,,Missing reward normalisation/scaling
SE 63,https://stackoverflow.com/questions/69134882,Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal Reward Function,,,Suboptimal reward function 
SE 65,https://ai.stackexchange.com/questions/6314,Suboptimal Discount Factor,*,Suboptimal Discount Factor,"""My solution has been to simply set ùõæ=1 in the PBRS term, even when using, say, ùõæ=0.99 in the RL formulation. "". This is a bit of a how to",Suboptimal Discount Factor,Discount factor is present but not in its standard meaning,Suboptimal Discount Factor,,Suboptimal Discount Factor,,,Suboptimal discount factor 
SE 66,https://ai.stackexchange.com/questions/9590,Missing target network,,Missing target network,"""By using something known as the target function approximator(target network), we can""",Missing target network,Missing discretization. The missing target network could also play a role.,Missing target network,,Missing target network,,,Drop
SE 68,https://ai.stackexchange.com/questions/10213,Suboptimal Discount Factor,,Suboptimal Discount Factor,,Suboptimal Discount Factor,missing discount factor,Suboptimal Discount Factor,,Suboptimal Discount Factor,,,Suboptimal discount factor 
SE 69,https://ai.stackexchange.com/questions/10306,Subtoptimal frame skip parameter,,Subtoptimal frame skip parameter,can't find evidence for this one,Subtoptimal frame skip parameter,Input preprocessing seems to be crucial here,Subtoptimal frame skip parameter,do we need a new label?,Subtoptimal frame skip parameter,"Many changes Used PongFrameskip-v4 instead of PongDeterministic-v4
Used a small replay buffer of size 10000
During a step_update() or replay() call",,Subtoptimal frame skip parameter
SE 69 [Duplicate],https://ai.stackexchange.com/questions/10306,Suboptimal Replay buffer size,,Suboptimal Replay Buffer Design,"""Used a small replay buffer of size 10000""",Suboptimal Replay buffer size,,Suboptimal Replay buffer size,,Suboptimal Replay buffer size,,,Suboptimal replay buffer size
SE 70,https://ai.stackexchange.com/questions/11924,Deep Learning Fault,,Deep Learning Fault,,Deep Learning Fault,,Deep Learning Fault,,Deep Learning Fault,,,Drop
SE 72 [Duplicate],https://ai.stackexchange.com/questions/13793,Deep Learning Fault,,Deep Learning Fault,Change of loss,Deep Learning Fault,,Deep Learning Fault,,Deep Learning Fault,,,Drop
SE 72 [Duplicate],https://ai.stackexchange.com/questions/13793,Missing normalisaton of observations,Normalize observations,Missing normalisaton of observations,"""100 time steps delay between rewards is not much for DQN"" - is this about reward?",Missing normalisaton of observations,,Missing normalisaton of observations,-,Missing normalisaton of observations,Check your input scaling,,Missing reward normalisation/scaling
SE 72 [Duplicate],https://ai.stackexchange.com/questions/13793,Missing replay buffer,,Missing replay buffer,,Missing replay buffer,,Missing replay buffer,,Missing replay buffer,experience replay,,Drop
SE 75,https://ai.stackexchange.com/questions/20267,Suboptimal learning rate,,Suboptimal learning rate,Can not find the label in the list ,Suboptimal learning rate,,Suboptimal learning rate,where is the lr label?,Suboptimal learning rate,suboptimal learning rate,,Drop
SE 77,https://ai.stackexchange.com/questions/26154,Missing exploration,,Missing exploration,"""You should be looking to act ùúñ-greedily, i.e. with probability ùúñ take a random action and act greedily otherwise. Typically you start with ùúñ=1 and decay it each time a random action is taken down to some small value such as 0.05.
""",Missing exploration,,Missing exploration,"many problems, discussion is not finished on SE",Missing exploration,,,Missing exploration 
SE 79,https://ai.stackexchange.com/questions/28548,Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal Reward Function,,Suboptimal Reward Function,,,Suboptimal reward function
SE 79  [Duplicate],https://ai.stackexchange.com/questions/28548,Reward Scale,,Reward Scale,,Reward Scale,,Reward Scale,,Reward Scale,,,Missing reward normalisation/scaling
SE 79  [Duplicate],https://ai.stackexchange.com/questions/28548,Deep Learning Fault,,Deep Learning Fault,Can not find the label for reward function,Deep Learning Fault,Learning rate,Deep Learning Fault,"not sure if it is not LR for DL, als the question was closed for the lack of clarity",Deep Learning Fault,,,Drop
SE 83,https://ai.stackexchange.com/questions/36627,Suboptimal Entropy Coefficient,,Suboptimal Entropy Coefficient,Can not find the label for entropy coefficient,Suboptimal Entropy Coefficient,,Suboptimal Entropy Coefficient,entropy coefficient? where is the label we had,Suboptimal Entropy Coefficient,,,Suboptimal entropy coefficient 
SE 84,https://datascience.stackexchange.com/questions/39052,Suboptimal features for state representation,,Suboptimal features for state representation,"The answer seems to suggest that the state representation is fine for RL. Overall, the answer seems too generic ",Suboptimal features for state representation,,Suboptimal features for state representation,,Suboptimal features for state representation,,,"Suboptimal features for state
representation "
SE 84 [Duplicate],https://datascience.stackexchange.com/questions/39052,Suboptimal scaling of features,,Suboptimal scaling of features,,Suboptimal scaling of features,,Suboptimal scaling of features,,Suboptimal scaling of features,,,Suboptimal scaling of features 
SE 84 [Duplicate],https://datascience.stackexchange.com/questions/39052,Incorrect done flag,"""The done flag discussion was fascinating as well, I wonder why there isn't more awareness of this issue. I don't think I have seen others mention it."" => I think the done flag should be included even if others aren't included",Incorrect done flag,,Incorrect done flag,,Incorrect done flag,,Incorrect done flag,"Describe many problem Does state representation generally affect how difficult a problem is? and flag done

",,"Flags indicating successful termination or
timeout not set properly"
SE 85,https://datascience.stackexchange.com/questions/56053,Suboptimal Replay Buffer Design,,Suboptimal Replay Buffer Design,,Suboptimal Replay Buffer Design,,Suboptimal Replay Buffer Design,,Suboptimal Replay Buffer Design,,,Suboptimal replay buffer design 
SE 86,https://stackoverflow.com/questions/31494361,Not enough episodes/iterations (training),Not enough iterations,Not enough episodes/iterations (training),"""Make a lot of iterations"" ",Not enough episodes/iterations (training),,Not enough episodes/iterations (training),,Not enough episodes/iterations (training),,,Not enough episodes/iterations (training)
SE 87,https://stackoverflow.com/questions/71786530,Incorrect done flag,"Maybe a better name, as it is a problem with the done variable",Incorrect done flag,"""There was an edge case where the environment was not ending""",Incorrect done flag,,Incorrect done flag,"not sure though, but cannot think of the better label",Incorrect done flag,,,"Flags indicating successful termination or
timeout not set properly "
SE 88,https://stackoverflow.com/questions/43382046,Large State Description,,Large State Description,,Large State Description,,Large State Description,,Large State Description,,,Suboptimal State Space
SE 88 [Duplicate],https://stackoverflow.com/questions/43382046,Suboptimal action space design,,Suboptimal action space design,,Suboptimal action space design,,Suboptimal action space design,,Suboptimal action space design,,,Suboptimal action space 
SE 88 [Duplicate],https://stackoverflow.com/questions/43382046,Sparse Reward,,Sparse Reward,Is it sparse?,Sparse Reward,,Sparse Reward,sparse reward is also fine..,Sparse Reward,,,Sparse reward 
SE 89,https://stackoverflow.com/questions/47750291,Missing exploration,,Missing exploration,"It is about epsilon, but there was not any exploration to begin with",Missing exploration,,Missing exploration,,Missing exploration,,,Missing exploration 
SE 93,https://stackoverflow.com/questions/68269032,Deep Learning Fault,,Deep Learning Fault,Can not find learning rate label,Deep Learning Fault,,Deep Learning Fault,,Deep Learning Fault,suboptimal learning rate,,Drop
SE 93 [Duplicate],https://stackoverflow.com/questions/68269032,Incorrect State Description,,Incorrect State Description,,Incorrect State Description,"""I changed the dimension of the states that been send to NN""; did not find the code though",Incorrect State Description,,Incorrect State Description,,,Suboptimal State Space
SE 94,https://stackoverflow.com/questions/70382999,Deep Learning Fault,,Deep Learning Fault,can not find learning rate label,Deep Learning Fault,Learning rate,Deep Learning Fault,,Deep Learning Fault,Suboptimal learning rate,,Drop
SE 95,https://stackoverflow.com/questions/72995715,Selecting a non-stochastic trained policy,,Selecting a non-stochastic trained policy,"""need to be very careful before making stochastic agents deterministic""",Selecting a non-stochastic trained policy,Selecting a non-stochastic trained policy?,Selecting a non-stochastic trained policy,Selecting a non-stochastic trained policy?,Selecting a non-stochastic trained policy,,,Misconfiguration of the agent to a deterministic inference

/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  deprecation(
/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  deprecation(
/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/gym/core.py:256: DeprecationWarning: [33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.[0m
  deprecation(
/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  deprecation(
/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  deprecation(
/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/gym/core.py:256: DeprecationWarning: [33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.[0m
  deprecation(
Traceback (most recent call last):
  File "test_agent.py", line 104, in <module>
    process.join()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
Process Process-21:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-18:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-27:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-29:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-17:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-31:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-30:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-13:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt

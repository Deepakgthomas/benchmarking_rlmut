Traceback (most recent call last):
  File "test_agent.py", line 104, in <module>
    process.join()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
Process Process-21:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-23:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-27:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-22:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-19:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-28:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-31:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-29:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-25:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-17:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-20:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-15:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-12:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-14:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-18:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-13:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-30:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-24:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Process Process-16:
Traceback (most recent call last):
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/thoma/benchmarking_rlmut/RLMutation/RLMT/agent.py", line 166, in test
    mean_reward, std_reward = evaluate_policy(
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 258, in predict
    action, state = self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 336, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 178, in _predict
    return self.q_net._predict(obs, deterministic=deterministic)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 69, in _predict
    q_values = self(observation)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs))
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/thoma/anaconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
